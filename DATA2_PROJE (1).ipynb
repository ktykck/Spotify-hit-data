{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Lasso, LassoCV, RidgeClassifier, RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we upload our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv(r\"dataset-of-90s.csv\") ; data_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We drop useless features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_raw.drop(['track', 'artist', 'uri'], axis=1) ;data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We change all values to float64 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype(float) ; data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are looking for describe function's features to understand our data better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, we are seperating our target and predictor set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,0:-1]\n",
    "y = data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, we are splitting our data to train and test set to get realistic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We perform logistic regression to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train, y_train)\n",
    "y_pred = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=pd.DataFrame({'True':y_test, 'Predicted':y_pred}) ; table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = accuracy_score(y_pred,y_test) ; score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we have seen our score as 0.4990942028985507 and we manually check the result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sum of '1''s is equal to:\", y_pred.sum(),\"at the prediction set\")\n",
    "print(\"Sum of '1''s is equal to:\", y_test.sum(),\"at the test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test.sum()/y_pred.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We use max_iter parameter but nothing changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression(max_iter=1000)\n",
    "log_model.fit(X_train, y_train)\n",
    "y_pred = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=pd.DataFrame({'True':y_test, 'Predicted':y_pred}) ; table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = accuracy_score(y_pred,y_test) ; score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we perform MinMaxScaler also known as Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "\n",
    "data_normalized = pd.DataFrame(data_normalized, columns = data.columns) ; data_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalized = data_normalized.drop('target',axis=1).astype(float)\n",
    "y = data_normalized[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_normalized, X_test_normalized, y_train, y_test =train_test_split(X_normalized, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train_normalized, y_train)\n",
    "y_pred = log_model.predict(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=pd.DataFrame({'True':y_test, 'Predicted':y_pred}) ;table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = precision_score(y_pred,y_test) ;score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfmatrix = confusion_matrix(y_test, y_pred) ;cfmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=cfmatrix) \n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, we perform standardization and logistic regression again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "X_stand = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stand, X_test_stand, y_train, y_test =train_test_split(X_stand, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train_stand, y_train)\n",
    "y_pred = log_model.predict(X_test_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=pd.DataFrame({'True':y_test, 'Predicted':y_pred}) ; table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = precision_score(y_pred,y_test) ;score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cfmatrix = confusion_matrix(y_test, y_pred) ; cfmatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=cfmatrix) \n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Until model selection. We reach best accuracy with standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First we perform on normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel = LogisticRegression()\n",
    "sbs = SFS(logmodel, \n",
    "           k_features=1,\n",
    "           forward=False, \n",
    "           floating=False, \n",
    "           verbose=2,\n",
    "           scoring='accuracy',\n",
    "           cv=10)\n",
    "feature_names = ('danceability', 'energy', 'key', 'loudness', 'mode','speechiness','acousticness','instrumentalness','liveness','valence','tempo','duration_ms','time_signature','chorus_hit','sections')\n",
    "sbs = sbs.fit(X_train_normalized, y_train, custom_feature_names=feature_names)\n",
    "pd.DataFrame.from_dict(sbs.get_metric_dict()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(sbs.get_metric_dict()).T.sort_values('avg_score',ascending=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we perform on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel = LogisticRegression()\n",
    "sbs = SFS(logmodel, \n",
    "           k_features=1,\n",
    "           forward=False, \n",
    "           floating=False, \n",
    "           verbose=2,\n",
    "           scoring='accuracy',\n",
    "           cv=10)\n",
    "feature_names = ('danceability', 'energy', 'key', 'loudness', 'mode','speechiness','acousticness','instrumentalness','liveness','valence','tempo','duration_ms','time_signature','chorus_hit','sections')\n",
    "sbs = sbs.fit(X_train, y_train, custom_feature_names=feature_names)\n",
    "pd.DataFrame.from_dict(sbs.get_metric_dict()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(sbs.get_metric_dict()).T.sort_values('avg_score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At last, we perform on standardized data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel = LogisticRegression()\n",
    "sbs = SFS(logmodel, \n",
    "           k_features=1,\n",
    "           forward=False, \n",
    "           floating=False, \n",
    "           verbose=2,\n",
    "           scoring='accuracy',\n",
    "           cv=10)\n",
    "feature_names = ('danceability', 'energy', 'key', 'loudness', 'mode','speechiness','acousticness','instrumentalness','liveness','valence','tempo','duration_ms','time_signature','chorus_hit','sections')\n",
    "sbs = sbs.fit(X_train_stand, y_train, custom_feature_names=feature_names)\n",
    "pd.DataFrame.from_dict(sbs.get_metric_dict()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(sbs.get_metric_dict()).T.sort_values('avg_score',ascending=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11)  Predictors gave us the best results with standardized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected = X.iloc[:,[0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11]]\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_selected_stand = scaler.fit_transform(X_selected)\n",
    "X_selected_stand = pd.DataFrame(X_selected_stand, columns = X_selected.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected_stand, X_test_selected_stand, y_train, y_test =train_test_split(X_selected_stand, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We perform logistic regression again with our new predictor set and we directly work with normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train_selected_stand, y_train)\n",
    "y_pred = log_model.predict(X_test_selected_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=pd.DataFrame({'True':y_test, 'Predicted':y_pred}) ; table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = precision_score(y_pred,y_test) ;score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfmatrix = confusion_matrix(y_test, y_pred) ;cfmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=cfmatrix) \n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We find best with accuracy 0.8185117967332124 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(5,-2,100)*0.5\n",
    "alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We perform for raw data, selected data, and scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgecv = RidgeClassifierCV(alphas = alphas,cv = 10, scoring = 'accuracy')\n",
    "ridgecv.fit(X_train, y_train)\n",
    "ridgecv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ridge = RidgeClassifier(alpha = ridgecv.alpha_)\n",
    "ridge.fit(X_train, y_train)\n",
    "print(\"Accuracy score for raw data is: \",accuracy_score(y_test, ridge.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgecv = RidgeClassifierCV(alphas = alphas,cv = 10, scoring = 'accuracy')\n",
    "ridgecv.fit(X_train_selected, y_train)\n",
    "ridgecv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = RidgeClassifier(alpha = ridgecv.alpha_)\n",
    "ridge.fit(X_train_selected, y_train)\n",
    "print(\"Accurarcy score for selected data is: \",accuracy_score(y_test, ridge.predict(X_test_selected)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgecv = RidgeClassifierCV(alphas = alphas,cv = 10, scoring = 'accuracy')\n",
    "ridgecv.fit(X_train_selected_stand, y_train)\n",
    "ridgecv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = RidgeClassifier(alpha = ridgecv.alpha_)\n",
    "ridge.fit(X_train_selected_stand, y_train)\n",
    "print(\"Accurarcy score for standardized data is: \",accuracy_score(y_test, ridge.predict(X_test_selected_stand)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a KNN classifier with 10 neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn= KNeighborsClassifier(n_neighbors= 10)\n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a KNN classifier with 10 neighbors with standardized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "X_stand = scaler.fit_transform(X)\n",
    "X_stand = pd.DataFrame(X_stand, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stand, X_test_stand , y_train, y_test = train_test_split(X_stand, y, test_size=0.2, random_state=0,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn= KNeighborsClassifier(n_neighbors= 10)\n",
    "knn.fit(X_train_stand,y_train)\n",
    "knn.predict(X_test_stand)\n",
    "knn.score(X_test_stand, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating KNN classifier with 65 neighbors and perform with both standardized and raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn= KNeighborsClassifier(n_neighbors= 65)\n",
    "knn.fit(X_train, y_train)\n",
    "knn.predict(X_test)\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn= KNeighborsClassifier(n_neighbors= 65)\n",
    "knn.fit(X_train_stand, y_train)\n",
    "knn.predict(X_test_stand)\n",
    "knn.score(X_test_stand, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Cross Validation for the best neighbor size with standardized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(1, 200)\n",
    "\n",
    "k_scores = []\n",
    "k_parameter = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train_stand, y_train, cv=10, scoring='accuracy') # It's 10 fold cross validation with 'accuracy' scoring\n",
    "    k_scores.append(scores.mean())\n",
    "    k_parameter.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = {'Validation Scores':k_scores,\n",
    "        'knn_parameter':k_parameter}\n",
    " \n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We find the best neighbor size as 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn= pd.DataFrame(k_scores, columns=['Validation Scores'])\n",
    "df_knn.sort_values(by='Validation Scores', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating KNN classifier with the optimum neighbor size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn= KNeighborsClassifier(n_neighbors=13)\n",
    "knn.fit(X_train_stand, y_train)\n",
    "knn.predict(X_test_stand)\n",
    "knn.score(X_test_stand, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=13)\n",
    "sbs = SFS(knn_model, \n",
    "           k_features=1,\n",
    "           forward=False, \n",
    "           floating=False, \n",
    "           verbose=2,\n",
    "           scoring='accuracy',\n",
    "           cv=10)\n",
    "feature_names = ('danceability', 'energy', 'key', 'loudness', 'mode','speechiness','acousticness','instrumentalness','liveness','valence','tempo','duration_ms','time_signature','chorus_hit','sections')\n",
    "sbs = sbs.fit(X_train_stand, y_train, custom_feature_names=feature_names)\n",
    "pd.DataFrame.from_dict(sbs.get_metric_dict()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(sbs.get_metric_dict()).T.sort_values('avg_score',ascending=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We define best subset is (0, 1, 6, 7, 9, 11)\t and we perform respect to these predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected = X.iloc[:,[0, 1, 6, 7, 9, 11]]\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_selected_stand = scaler.fit_transform(X_selected)\n",
    "X_selected_stand = pd.DataFrame(X_selected_stand, columns = X_selected.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected_stand, X_test_selected_stand, y_train, y_test =train_test_split(X_selected_stand, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn= KNeighborsClassifier(n_neighbors=13)\n",
    "knn.fit(X_train_selected_stand, y_train)\n",
    "knn.predict(X_test_selected_stand)\n",
    "knn.score(X_test_selected_stand, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desicion Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are creating adecision tree without any penalty at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_col=X.columns ; X_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(random_state = 0)  \n",
    "classifier.fit(X_train, y_train) \n",
    "fig = plt.figure(figsize=(20,15))\n",
    "Dtree = tree.plot_tree(classifier, feature_names=X_col, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a range for alpha values that we are going to use for adding some penalty to number of internal nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(random_state = 0) \n",
    "path = classifier.cost_complexity_pruning_path(X_train, y_train) ;path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are using cross validation to determine the optimal level of complexity of three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10, random_state=0, shuffle=True)\n",
    "CVErrors=[]\n",
    "for i in path.ccp_alphas:\n",
    "    for train_index, validation_index in cv.split(X_train):\n",
    "        X_trainp, X_valid = X_train.iloc[train_index], X_train.iloc[validation_index]\n",
    "        y_trainp, y_valid = y_train.iloc[train_index], y_train.iloc[validation_index]\n",
    "        classifier = DecisionTreeClassifier(random_state = 0, ccp_alpha=i)\n",
    "        classifier.fit(X_trainp, y_trainp) \n",
    "        y_pred=classifier.predict(X_valid)\n",
    "        CVErrors.append([i,accuracy_score(y_trainp, classifier.predict(X_trainp)),accuracy_score(y_valid, y_pred)])    \n",
    "df = pd.DataFrame(CVErrors,columns=['alpha','Training Accuracy','Validation Accuracy'])\n",
    "kfoldCV_by_alpha = df.groupby('alpha')\n",
    "kfoldCV_by_alpha = kfoldCV_by_alpha.mean()\n",
    "kfoldCV_by_alpha = kfoldCV_by_alpha.reset_index()\n",
    "kfoldCV_by_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are sorting alpha values to find one which gives us best validation accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfoldCV_by_alpha=kfoldCV_by_alpha.sort_values(by=['Validation Accuracy'],ascending=False) ; kfoldCV_by_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we are creating a new decision tree with the best alpha value which gives us best validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(random_state = 0,ccp_alpha=0.000997)  \n",
    "classifier.fit(X_train, y_train) \n",
    "fig = plt.figure(figsize=(20,15))\n",
    "Dtree = tree.plot_tree(classifier, class_names=['Non-Hit','Hit'],feature_names=X_col, filled=True)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp=pd.DataFrame({'Actual':y_test, 'Predicted':y_pred})\n",
    "comp = pd.concat([X_test.reset_index(drop=True), comp.reset_index(drop=True)], axis= 1) ; comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we are going to fit a random forest / we are trying different numbers of trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Oob_Accuracy=[]\n",
    "for i in np.linspace(start = 100, stop = 1000, num = 10):\n",
    "    clf=RandomForestClassifier(random_state=0,n_estimators=int(i),oob_score=True)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    Oob_Accuracy.append([i,np.array(clf.oob_score_)])\n",
    "df = pd.DataFrame(Oob_Accuracy,columns=['Number_of_Trees','Oob Accuracy'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values(by=['Oob Accuracy'],ascending=False) ; df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are plotting a scatter graph to pick the number of trees that gives the highest accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(5,4), dpi=100)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter (df['Number_of_Trees'].values, df['Oob Accuracy'].values, label = 'Oob Accuracy')\n",
    "ax.set_xlabel('Number_of_Trees')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.tick_params(axis='x', labelsize=8)\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(random_state=0,n_estimators=600,oob_score=True)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are trying different numbers of features at each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Oob_Accuracy=[]\n",
    "for i in range(1,16):\n",
    "    clf=RandomForestClassifier(random_state=0,n_estimators=600,max_features=i,oob_score=True)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    Oob_Accuracy.append([i,np.array(clf.oob_score_)])\n",
    "df = pd.DataFrame(Oob_Accuracy,columns=['Number_of_Features','Oob Accuracy'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values(by=['Oob Accuracy'],ascending=False) ; df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are plotting a scatter graph to pick the number of features that gives the highest accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(5,4), dpi=100)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(df['Number_of_Features'].values,df['Oob Accuracy'].values,label = 'Oob Accuracy')\n",
    "ax.set_xlabel('Number_of_Features')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.tick_params(axis='x', labelsize=8)\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are creating a new random forest with the best parameters that we choose above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(random_state=0,n_estimators=600,max_features=5,oob_score=True)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print('Test Accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are forming a confusion matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test, y_pred) ; cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this will visualize the confusion matrix and help us to interpret our model's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm) \n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(random_state=0)\n",
    "# number of trees in random forest\n",
    "n_estimators = [100,200,300,400,500,600]\n",
    "# number of features at every split\n",
    "max_features = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "# create grid\n",
    "params = {\n",
    " 'n_estimators': n_estimators,\n",
    " 'max_features': max_features,\n",
    " }\n",
    "\n",
    "#We are using a nested cross-validation for both parameter selection and the test performance\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "CVAccuracy=[]\n",
    "for train_index, validation_index in cv.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[validation_index], \n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[validation_index]\n",
    "    # Grid search of parameters\n",
    "    clf_grid = GridSearchCV(estimator = clf, param_grid = params, \n",
    "                                cv = 5, verbose=2, scoring='accuracy',n_jobs = -1)\n",
    "    # Fit the model\n",
    "    clf_grid.fit(X_train, y_train)\n",
    "    # print results\n",
    "    print(clf_grid.best_params_)\n",
    "    #After finding best parameters fit the model\n",
    "    clf=RandomForestClassifier(**clf_grid.best_params_)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    #Test the performance on the test set\n",
    "    CVAccuracy.append(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CVAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(CVAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are plotting the impurity-based feature importances of the forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = {} # a dict to hold feature_name: feature_importance\n",
    "importances = clf.feature_importances_\n",
    "X_col=X.columns\n",
    "for feature, importance in zip(X_col, clf.feature_importances_):\n",
    "    feats[feature] = importance #add the name/value pair \n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n",
    "importances = importances.sort_values(by='Gini-importance',ascending=False)\n",
    "print(importances)\n",
    "sorted_idx = clf.feature_importances_.argsort()\n",
    "plt.barh(X_col[sorted_idx], clf.feature_importances_[sorted_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
